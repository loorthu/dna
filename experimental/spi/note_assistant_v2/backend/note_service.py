import os
import random
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel

router = APIRouter()

class LLMSummaryRequest(BaseModel):
    text: str

# --- LLM IMPLEMENTATION CODE ---

import os
import requests
from openai import OpenAI
import anthropic
import google.generativeai as genai
import re
from dotenv import load_dotenv

# === CONFIGURABLE PARAMETERS ===
TEMPERATURE = 0.1

SYSTEM_PROMPT = """
You are a helpful assistant that reviews audio transcripts of meetings and recreates short and precise abbreviated conversations between people from them.
The meeting is about discussion of creative work submissions by artists, who are working on parts of a movie (called 'shots').
The intent of the meeting is to review those submissions one by one and provide clear feedback and suggestions to the artist.
The abbreviated conversations generated by you are meant for the artists to quickly read to get the gist of exactly what was said by who and understand
the next steps, if any.
"""

USER_PROMPT_TEMPLATE = """
The following is a conversation about a shot.

Create notes on specific creative decisions and any actionable tasks for each of them. Be as concisie and direct as possible in the notes.
You can include the speaker initials in the notes to identify the person who made the point. Highlight any important decisions reached, 
such as a shot being approved (finalled) by the creative lead.

Just generate short/consise notes from the given conversation, without any header or footer text such as subject line or follow up questions.

Following is the conversation:
{conversation}

"""

DEFAULT_MODELS = {
    "openai": "gpt-4o",
    "claude": "claude-3-sonnet-20240229",
    "ollama": "llama3.2",
    "gemini": "gemini-2.5-flash-preview-05-20"
}

def summarize_openai(conversation, model, client):
    prompt = USER_PROMPT_TEMPLATE.format(conversation=conversation)
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ],
        temperature=TEMPERATURE,
    )
    return response.choices[0].message.content

def summarize_claude(conversation, model, client):
    prompt = USER_PROMPT_TEMPLATE.format(conversation=conversation)
    response = client.messages.create(
        model=model,
        max_tokens=1024,
        temperature=TEMPERATURE,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ]
    )
    return response.content[0].text

def summarize_ollama(conversation, model, client):
    prompt = SYSTEM_PROMPT + "\n\n" + USER_PROMPT_TEMPLATE.format(conversation=conversation)
    response = client.post(
        "http://localhost:11434/api/generate",
        json={"model": model, "prompt": prompt, "stream": False}
    )
    return response.json()["response"]

def summarize_gemini(conversation, model, client):
    full_prompt = f"{SYSTEM_PROMPT}\n\n{USER_PROMPT_TEMPLATE.format(conversation=conversation)}"
    response = client.generate_content(
        full_prompt,
        generation_config=genai.types.GenerationConfig(
            max_output_tokens=1024,
            temperature=TEMPERATURE,
        )
    )
    if not response.candidates:
        raise Exception("No response candidates returned from Gemini")
    candidate = response.candidates[0]
    if candidate.finish_reason == 2:
        raise Exception("Response blocked by Gemini safety filters")
    elif candidate.finish_reason == 3:
        raise Exception("Response blocked due to recitation concerns")
    elif candidate.finish_reason == 4:
        raise Exception("Response blocked for other reasons")
    if not candidate.content or not candidate.content.parts:
        raise Exception("No content parts in response")
    return candidate.content.parts[0].text

def create_llm_client(provider, api_key=None, model=None):
    provider = provider.lower()
    if provider == "openai":
        if not api_key:
            raise ValueError("OpenAI requires an api_key.")
        return OpenAI(api_key=api_key)
    elif provider == "claude":
        if not api_key:
            raise ValueError("Anthropic Claude requires an api_key.")
        return anthropic.Anthropic(api_key=api_key)
    elif provider == "ollama":
        return requests.Session()
    elif provider == "gemini":
        if not api_key:
            raise ValueError("Gemini requires an api_key.")
        if not model:
            raise ValueError("Gemini requires a model name.")
        genai.configure(api_key=api_key)
        return genai.GenerativeModel(model)
    else:
        raise ValueError(f"Unsupported provider: {provider}")

# --- Gemini LLM client cache ---
gemini_api_key = os.getenv("GEMINI_API_KEY")
gemini_model = DEFAULT_MODELS["gemini"]
gemini_client = None
if gemini_api_key:
    try:
        gemini_client = create_llm_client("gemini", api_key=gemini_api_key, model=gemini_model)
    except Exception as e:
        print(f"Error initializing Gemini client: {e}")

DISABLE_LLM = os.getenv('DISABLE_LLM', 'true').lower() in ('1', 'true', 'yes')

@router.post("/llm-summary")
async def llm_summary(data: LLMSummaryRequest):
    """
    Generate a summary using Gemini LLM for the given text.
    """
    if DISABLE_LLM:
        # Return a random summary for testing
        random_summaries = [
            "The team discussed lighting and animation improvements.",
            "Minor tweaks needed for character animation; background approved.",
            "Action items: soften shadows, adjust highlight gain, improve hand motion.",
            "Most notes addressed; only a few minor issues remain.",
            "Ready for final review after next round of changes.",
            "Feedback: color grade is close, but highlights too hot.",
            "Artist to be notified about animation and lighting feedback.",
            "Overall progress is good; next steps communicated to the team."
        ]
        return {"summary": random.choice(random_summaries)}
    try:
        if not gemini_client:
            raise HTTPException(status_code=500, detail="Gemini client not initialized.")
        summary = summarize_gemini(data.text, gemini_model, gemini_client)
        return {"summary": summary}
    except Exception as e:
        print(f"Error in /llm-summary: {e}")
        raise HTTPException(status_code=500, detail=f"LLM summary error: {str(e)}")