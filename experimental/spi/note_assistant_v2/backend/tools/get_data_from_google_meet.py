#!/usr/bin/env python3
"""
Google Meet Data Extractor

This script processes Google Meet video recordings to extract synchronized audio transcripts, speaker names, 
and version IDs, then groups them into coherent speaking segments. It combines functionality 
from both audio transcription and visual text detection to create a unified output.

Key Features:
- Audio transcription using OpenAI's Whisper model
- Speaker name detection using OCR on video frames 
- Version ID detection using regex pattern matching (mandatory)
- Intelligent grouping of transcript segments by speaker and version
- Synchronized output with grouped speaking turns
- True parallel processing using multiprocessing (bypasses Python GIL)

Output Format:
CSV with columns: timestamp, transcript_text, speaker_name, version_id
- Each row represents a complete speaking turn by one person
- Consecutive segments from the same speaker with same version are combined
- Segments are split when speaker changes or version ID changes

Usage Examples:
    # Basic processing with local file
    python get_data_from_google_meet.py meeting.mp4 --version-pattern "v\\d+\\.\\d+\\.\\d+"

    # Google Drive URL
    python get_data_from_google_meet.py "https://drive.google.com/file/d/1a2b3c4d/view" --version-pattern "v\\d+\\.\\d+\\.\\d+" --drive-credentials /path/to/service_account.json

    # Google Drive file ID (OAuth2 - will prompt for login on first run)
    python get_data_from_google_meet.py 1a2b3c4d5e6f7g8h9i --version-pattern "goat-\\d+" --parallel

    # Custom audio model and frame interval
    python get_data_from_google_meet.py meeting.mp4 --version-pattern "goat-\\d+" --audio-model small --frame-interval 3.0

    # Process specific time range
    python get_data_from_google_meet.py meeting.mp4 --version-pattern "v\\d+\\.\\d+\\.\\d+" --start-time 120 --duration 300

Requirements:
- All dependencies from get_audio_transcript.py and get_onscreen_text.py
- FFmpeg for video processing
- OpenAI Whisper for audio transcription
- EasyOCR for text detection
- OpenCV and PIL for image processing
"""

import argparse
import os
import re
import csv
import tempfile
import subprocess
from typing import List, Dict, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import threading
import multiprocessing

# Import functions from existing scripts
from get_audio_transcript import process_media_file
from get_onscreen_text import (
    process_video as process_video_visual,
    get_video_duration,
    sanitize_speaker_names
)


def _run_audio_transcription_worker(video_path: str, transcript_csv: str, 
                                   audio_model: str, duration: Optional[float], verbose: bool) -> bool:
    """
    Worker function for audio transcription in multiprocessing.
    Must be defined at module level for multiprocessing to work.
    """
    if verbose:
        print("[Audio Process] Starting audio transcription...")
    return process_media_file(
        video_path,
        transcript_csv,
        model_name=audio_model,
        duration=duration,
        verbose=verbose
    )


def _run_visual_detection_worker(video_path: str, frame_interval: float, visual_csv: str,
                                duration: Optional[float], batch_size: int, verbose: bool,
                                version_pattern: str, start_time: float) -> bool:
    """
    Worker function for visual detection in multiprocessing.
    Must be defined at module level for multiprocessing to work.
    """
    if verbose:
        print("[Visual Process] Starting visual detection...")
    return process_video_visual(
        video_path,
        frame_interval,
        visual_csv,
        max_duration=duration,
        batch_size=batch_size,
        verbose=verbose,
        version_pattern=version_pattern,  # Always required
        start_time=start_time,
        parallel=True  # Enable parallel frame processing within visual detection too
    )


def parse_transcript_csv(transcript_csv_path: str) -> List[Dict]:
    """
    Parse the transcript CSV file generated by get_audio_transcript.py
    
    Args:
        transcript_csv_path: Path to the transcript CSV file
        
    Returns:
        List of dictionaries with transcript segments
    """
    transcript_segments = []
    
    if not os.path.exists(transcript_csv_path):
        return transcript_segments
    
    try:
        with open(transcript_csv_path, 'r', encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                transcript_segments.append({
                    'start_time': float(row['start_time']),
                    'end_time': float(row['end_time']),
                    'text': row['text'].strip()
                })
    except Exception as e:
        print(f"Error reading transcript CSV: {e}")
    
    return transcript_segments


def parse_visual_csv(visual_csv_path: str) -> List[Dict]:
    """
    Parse the visual detection CSV file generated by get_onscreen_text.py
    
    Args:
        visual_csv_path: Path to the visual detection CSV file
        
    Returns:
        List of dictionaries with visual detections
    """
    visual_detections = []
    
    if not os.path.exists(visual_csv_path):
        return visual_detections
    
    try:
        with open(visual_csv_path, 'r', encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile)
            for row in reader:
                # Convert HH:MM:SS timestamp to seconds
                timestamp_str = row['timestamp']
                time_parts = timestamp_str.split(':')
                timestamp_seconds = int(time_parts[0]) * 3600 + int(time_parts[1]) * 60 + int(time_parts[2])
                
                visual_detections.append({
                    'timestamp': timestamp_seconds,
                    'speaker_name': row.get('speaker_name', '').strip(),
                    'version_id': row.get('version_id', '').strip()
                })
    except Exception as e:
        print(f"Error reading visual CSV: {e}")
    
    return visual_detections


def find_nearest_visual_detection(transcript_time: float, visual_detections: List[Dict]) -> Dict:
    """
    Find the visual detection closest in time to a transcript segment
    
    Args:
        transcript_time: Time in seconds from transcript segment
        visual_detections: List of visual detection records
        
    Returns:
        Dictionary with nearest speaker_name and version_id, or empty strings
    """
    if not visual_detections:
        return {'speaker_name': '', 'version_id': ''}
    
    # Find the detection with minimum time difference
    min_diff = float('inf')
    nearest_detection = {'speaker_name': '', 'version_id': ''}
    
    for detection in visual_detections:
        time_diff = abs(detection['timestamp'] - transcript_time)
        if time_diff < min_diff:
            min_diff = time_diff
            nearest_detection = {
                'speaker_name': detection['speaker_name'],
                'version_id': detection['version_id']
            }
    
    return nearest_detection


def synchronize_data(transcript_segments: List[Dict], visual_detections: List[Dict]) -> List[Dict]:
    """
    Synchronize transcript segments with visual detections
    
    Args:
        transcript_segments: List of transcript segments from audio
        visual_detections: List of visual detections from frames
        
    Returns:
        List of synchronized records with transcript, speaker, and version data
    """
    synchronized_records = []
    
    for segment in transcript_segments:
        # Use the middle of the transcript segment for matching
        segment_mid_time = (segment['start_time'] + segment['end_time']) / 2
        
        # Find nearest visual detection
        nearest_visual = find_nearest_visual_detection(segment_mid_time, visual_detections)
        
        synchronized_records.append({
            'start_time': segment['start_time'],
            'end_time': segment['end_time'],
            'text': segment['text'],
            'speaker_name': nearest_visual['speaker_name'],
            'version_id': nearest_visual['version_id']
        })
    
    return synchronized_records


def group_by_speaker_and_version(synchronized_records: List[Dict]) -> List[Dict]:
    """
    Group consecutive transcript segments by speaker and version ID
    
    Args:
        synchronized_records: List of synchronized transcript records
        
    Returns:
        List of grouped records with concatenated transcript text
    """
    if not synchronized_records:
        return []
    
    grouped_records = []
    current_group = {
        'start_time': synchronized_records[0]['start_time'],
        'text_segments': [synchronized_records[0]['text']],
        'speaker_name': synchronized_records[0]['speaker_name'],
        'version_id': synchronized_records[0]['version_id']
    }
    
    for record in synchronized_records[1:]:
        # Check if this record should be grouped with the current group
        same_speaker = (record['speaker_name'] == current_group['speaker_name'])
        same_version = (record['version_id'] == current_group['version_id'])
        
        if same_speaker and same_version:
            # Add to current group
            current_group['text_segments'].append(record['text'])
        else:
            # Finish current group and start new one
            grouped_records.append({
                'start_time': current_group['start_time'],
                'transcript_text': ' '.join(current_group['text_segments']),
                'speaker_name': current_group['speaker_name'],
                'version_id': current_group['version_id']
            })
            
            # Start new group
            current_group = {
                'start_time': record['start_time'],
                'text_segments': [record['text']],
                'speaker_name': record['speaker_name'],
                'version_id': record['version_id']
            }
    
    # Don't forget the last group
    grouped_records.append({
        'start_time': current_group['start_time'],
        'transcript_text': ' '.join(current_group['text_segments']),
        'speaker_name': current_group['speaker_name'],
        'version_id': current_group['version_id']
    })
    
    return grouped_records


def format_timestamp(seconds: float) -> str:
    """
    Convert seconds to HH:MM:SS format
    
    Args:
        seconds: Time in seconds
        
    Returns:
        Formatted timestamp string
    """
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    return f"{hours:02d}:{minutes:02d}:{secs:02d}"


def extract_google_meet_data(video_path: str, version_pattern: str, output_csv: str,
                           audio_model: str = "base", frame_interval: float = 5.0,
                           start_time: float = 0.0, duration: Optional[float] = None,
                           batch_size: int = 20, verbose: bool = False, parallel: bool = False,
                           drive_credentials: Optional[str] = None) -> bool:
    """
    Extract data from a Google Meet recording: synchronized transcripts, speaker names, and version IDs

    Supports both local video files and Google Drive URLs/file IDs. When a Drive URL is detected,
    the file is automatically downloaded to a temporary location before processing.

    Args:
        video_path: Path to local video file OR Google Drive URL/file ID
        version_pattern: Regex pattern for version ID detection
        output_csv: Path to the output CSV file
        audio_model: Whisper model to use for transcription
        frame_interval: Interval between frame extractions for visual detection
        start_time: Start processing from this time offset
        duration: Maximum duration to process
        verbose: Print detailed progress information
        parallel: Enable parallel processing (audio + visual simultaneously using multiprocessing)
        drive_credentials: Path to Google Drive OAuth2 credentials JSON (default: ../client_secret.json)

    Returns:
        True if successful, False otherwise
    """
    # Import Google Drive utilities
    from google_drive_utils import parse_drive_url, download_drive_file

    # Detect if input is a Google Drive URL/ID
    file_id = parse_drive_url(video_path)
    temp_video_path = None
    temp_video_dir = None

    if file_id:
        # Download from Google Drive
        if verbose:
            print(f"Detected Google Drive file ID: {file_id}")

        # Set default credentials path if not provided (OAuth2 in parent directory)
        if drive_credentials is None:
            drive_credentials = os.path.join(os.path.dirname(__file__), '..', 'client_secret.json')

        # Create temp directory for download
        temp_video_dir = tempfile.mkdtemp(prefix="google_drive_download_")
        temp_video_path = os.path.join(temp_video_dir, "video.mp4")

        try:
            if verbose:
                print(f"Downloading from Google Drive to: {temp_video_path}")

            success = download_drive_file(file_id, temp_video_path, drive_credentials, verbose)

            if not success:
                print("Failed to download file from Google Drive")
                if os.path.exists(temp_video_dir):
                    import shutil
                    shutil.rmtree(temp_video_dir)
                return False

            # Update video_path to point to downloaded file
            video_path = temp_video_path

        except Exception as e:
            print(f"Error downloading from Google Drive: {e}")
            if os.path.exists(temp_video_dir):
                import shutil
                shutil.rmtree(temp_video_dir)
            return False

    # Check if video file exists (works for both local and downloaded files)
    if not os.path.exists(video_path):
        print(f"Error: Video file not found at '{video_path}'")
        return False
    
    if verbose:
        print(f"Processing Google Meet recording: {video_path}")
        print(f"Version pattern: {version_pattern}")
        print(f"Audio model: {audio_model}")
        print(f"Frame interval: {frame_interval}s")
        if start_time > 0:
            print(f"Start time: {start_time}s")
        if duration:
            print(f"Duration limit: {duration}s")
    
    # Create temporary directory for intermediate files
    temp_dir = tempfile.mkdtemp(prefix="google_meet_data_")
    
    try:
        # Define paths for intermediate CSV files
        transcript_csv = os.path.join(temp_dir, "transcript.csv")
        visual_csv = os.path.join(temp_dir, "visual.csv")
        
        if verbose:
            print(f"Using temporary directory: {temp_dir}")
        
        if parallel:
            # Run audio transcription and visual detection in parallel using multiprocessing
            if verbose:
                print("\n=== Running Audio Transcription and Visual Detection in Parallel (Multiprocessing) ===")
            
            # Use ProcessPoolExecutor for true parallelism (bypasses GIL)
            try:
                with ProcessPoolExecutor(max_workers=2) as executor:
                    # Submit both tasks to separate processes
                    audio_future = executor.submit(
                        _run_audio_transcription_worker,
                        video_path, transcript_csv, audio_model, duration, verbose
                    )
                    visual_future = executor.submit(
                        _run_visual_detection_worker,
                        video_path, frame_interval, visual_csv, duration, batch_size, 
                        verbose, version_pattern, start_time
                    )
                    
                    # Wait for both to complete and get results
                    transcript_success = audio_future.result()
                    visual_success = visual_future.result()
            except Exception as e:
                print(f"Error in multiprocessing execution: {e}")
                print("Falling back to sequential processing...")
                # Fall back to sequential processing if multiprocessing fails
                return extract_google_meet_data(
                    video_path, version_pattern, output_csv, audio_model, 
                    frame_interval, start_time, duration, batch_size, verbose, parallel=False
                )
            
            if not transcript_success:
                print("Failed to generate audio transcript")
                return False
            
            if not visual_success:
                print("Failed to generate visual detections")
                return False
            
            if verbose:
                print("[Multiprocessing] Both audio transcription and visual detection completed successfully")
                print(f"Audio transcript saved to: {transcript_csv}")
                print(f"Visual detections saved to: {visual_csv}")
        
        else:
            # Sequential processing (original behavior)
            # Step 1: Extract audio transcript
            if verbose:
                print("\n=== Step 1: Audio Transcription ===")
            
            transcript_success = process_media_file(
                video_path,
                transcript_csv,
                model_name=audio_model,
                duration=duration,
                verbose=verbose
            )
            
            if not transcript_success:
                print("Failed to generate audio transcript")
                return False
            
            if verbose:
                print(f"Audio transcript saved to: {transcript_csv}")
            
            # Step 2: Extract visual detections (speaker names + version IDs)
            if verbose:
                print("\n=== Step 2: Visual Detection ===")
            
            visual_success = process_video_visual(
                video_path,
                frame_interval,
                visual_csv,
                max_duration=duration,
                batch_size=batch_size,
                verbose=verbose,
                version_pattern=version_pattern,  # Always required
                start_time=start_time,
                parallel=False  # Sequential frame processing
            )
            
            if not visual_success:
                print("Failed to generate visual detections")
                return False
            
            if verbose:
                print(f"Visual detections saved to: {visual_csv}")
        
        # Step 3: Parse and synchronize data
        if verbose:
            print("\n=== Step 3: Data Synchronization ===")
        
        transcript_segments = parse_transcript_csv(transcript_csv)
        visual_detections = parse_visual_csv(visual_csv)
        
        if verbose:
            print(f"Parsed {len(transcript_segments)} transcript segments")
            print(f"Parsed {len(visual_detections)} visual detections")
        
        if not transcript_segments:
            print("No transcript segments found")
            return False
        
        # Synchronize transcript with visual data
        synchronized_records = synchronize_data(transcript_segments, visual_detections)
        
        if verbose:
            print(f"Created {len(synchronized_records)} synchronized records")
        
        # Step 4: Group by speaker and version
        if verbose:
            print("\n=== Step 4: Grouping by Speaker and Version ===")
        
        grouped_records = group_by_speaker_and_version(synchronized_records)
        
        if verbose:
            print(f"Grouped into {len(grouped_records)} speaking turns")
        
        # Step 5: Apply speaker name sanitization
        if verbose:
            print("\n=== Step 5: Speaker Name Sanitization ===")
        
        # Extract speaker names for sanitization
        speaker_names = [record['speaker_name'] for record in grouped_records]
        timestamps_for_sanitization = [format_timestamp(record['start_time']) for record in grouped_records]
        
        sanitized_names = sanitize_speaker_names(timestamps_for_sanitization, speaker_names)
        
        # Update records with sanitized names
        for i, record in enumerate(grouped_records):
            record['speaker_name'] = sanitized_names[i]
        
        if verbose:
            unique_speakers = set(name for name in sanitized_names if name)
            print(f"Found {len(unique_speakers)} unique speakers: {list(unique_speakers)}")
        
        # Step 6: Write final CSV output
        if verbose:
            print(f"\n=== Step 6: Writing Output ===")
        
        with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['timestamp', 'transcript_text', 'speaker_name', 'version_id'])
            
            for record in grouped_records:
                timestamp_str = format_timestamp(record['start_time'])
                writer.writerow([
                    timestamp_str,
                    record['transcript_text'],
                    record['speaker_name'],
                    record['version_id']
                ])
        
        print(f"\nResults saved to: {output_csv}")
        print(f"Generated {len(grouped_records)} grouped speaking turns")
        
        # Summary statistics
        segments_with_speakers = sum(1 for record in grouped_records if record['speaker_name'])
        segments_with_versions = sum(1 for record in grouped_records if record['version_id'])
        
        print(f"Speaking turns with detected speakers: {segments_with_speakers}")
        print(f"Speaking turns with detected versions: {segments_with_versions}")
        
        return True
        
    except Exception as e:
        print(f"Error during processing: {e}")
        return False
    
    finally:
        # Clean up temporary directory for intermediate files
        import shutil
        if os.path.exists(temp_dir):
            try:
                shutil.rmtree(temp_dir)
                if verbose:
                    print(f"Cleaned up temporary directory: {temp_dir}")
            except Exception as e:
                print(f"Warning: Failed to clean up temporary directory: {e}")

        # Clean up downloaded Google Drive file
        if temp_video_dir and os.path.exists(temp_video_dir):
            try:
                shutil.rmtree(temp_video_dir)
                if verbose:
                    print(f"Cleaned up downloaded file: {temp_video_path}")
            except Exception as e:
                print(f"Warning: Failed to clean up downloaded file: {e}")


def main():
    """
    Main function to handle command-line arguments for Google Meet data extraction
    """
    parser = argparse.ArgumentParser(
        description="Extract data from Google Meet recordings: synchronized transcripts, speaker names, and version IDs."
    )
    
    # Required arguments
    parser.add_argument("input_video",
                       help="Path to the input video file (.mp4, .avi, .mov, etc.) OR Google Drive URL/file ID. "
                            "Supported Drive formats: https://drive.google.com/file/d/FILE_ID/view OR FILE_ID")
    parser.add_argument("--version-pattern", required=True, type=str,
                       help="Regex pattern to detect version IDs (e.g., 'v\\d+\\.\\d+\\.\\d+' for version numbers, 'goat-\\d+' for build numbers).")
    
    # Optional arguments
    parser.add_argument("-o", "--output", help="Output CSV file path. If not provided, will be input filename with '_processed.csv' suffix.")
    parser.add_argument("--audio-model", default="base", 
                       choices=["tiny", "base", "small", "medium", "large"],
                       help="Whisper model to use for audio transcription (default: base).")
    parser.add_argument("--frame-interval", "--interval", type=float, default=5.0,
                       help="Time interval in seconds between frame extractions for visual detection (default: 5.0).")
    parser.add_argument("--batch-size", type=int, default=20,
                       help="Number of frames to process in each batch for visual detection (default: 20).")
    parser.add_argument("--start-time", type=float, default=0.0,
                       help="Start processing from this time offset in seconds (default: 0.0).")
    parser.add_argument("--duration", type=float,
                       help="Maximum duration in seconds to process. If not specified, processes the entire video.")
    parser.add_argument("-v", "--verbose", action="store_true",
                       help="Print detailed progress information.")
    parser.add_argument("--parallel", action="store_true",
                       help="Enable parallel processing: run audio transcription and visual detection simultaneously using multiprocessing (faster but uses more CPU/memory resources). Bypasses Python GIL for better performance on multi-core systems.")
    parser.add_argument("--drive-credentials", type=str, default=None,
                       help="Path to Google Drive OAuth2 credentials JSON. Required for Drive URLs. Defaults to ../client_secret.json if not specified.")

    args = parser.parse_args()
    
    # Determine output file path
    if args.output:
        output_csv = args.output
    else:
        base_name = os.path.splitext(args.input_video)[0]
        output_csv = f"{base_name}_processed.csv"
    
    # Process the recording
    success = extract_google_meet_data(
        args.input_video,
        args.version_pattern,
        output_csv,
        audio_model=args.audio_model,
        frame_interval=args.frame_interval,
        start_time=args.start_time,
        duration=args.duration,
        batch_size=args.batch_size,
        verbose=args.verbose,
        parallel=args.parallel,
        drive_credentials=args.drive_credentials
    )
    
    if not success:
        exit(1)


if __name__ == "__main__":
    # Set multiprocessing start method for better cross-platform compatibility
    # 'spawn' is safer for complex imports but 'fork' is faster on Unix-like systems
    try:
        if hasattr(os, 'fork'):
            # Unix-like systems: use 'fork' for better performance
            multiprocessing.set_start_method('fork', force=True)
        else:
            # Windows: must use 'spawn'
            multiprocessing.set_start_method('spawn', force=True)
    except RuntimeError:
        # Start method already set, continue
        pass
    main()
